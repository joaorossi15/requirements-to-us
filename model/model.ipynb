{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import peft\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "model_name = 'tiiuae/falcon-7b'\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name: str):\n",
    "    m = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map='cuda',\n",
    "        trust_remote_code=False,\n",
    "        revision='main',\n",
    "    )\n",
    "\n",
    "    return m.to(device)\n",
    "\n",
    "model_name = \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\n",
    "m = load_model(model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_dataset(model_name: str):\n",
    "    df = pd.read_csv('/content/data_final.csv')\n",
    "    df = df[['text', 'is_major_principle_here', 'ethical_us']]\n",
    "    transparency = \"Transparency: Transparency can typically be understood in two ways: the transparency of the AI technology itself and the transparency of the AI organisations developing and using it. Throughout our analysis, transparency was regularly discussed directly, or in relation to processes required to ensure it, such as explainability, understandability and communication.\"\n",
    "    n_malef = \"Non-maleficence: The principle of non-maleficence gained attention, and in its most basic form, it means to do no harm or avoid doing harm to others\"\n",
    "    resp = \"Responsibility: The principle of responsibility is base on responsibility of the developers and stakeholders over the AI, accountability, liability and acting with integrity\"\n",
    "    privacy = \"Privacy: Related to privacy of personal data, because of the large abundance of data that is required for AI to work, it is important that individuals privacy is not jeopardised as a result\"\n",
    "    benefiecnce = \"Beneficence: Beneficence essentially means to do good, to carry out an activity with the intention of benefitting someone or society as a whole\"\n",
    "    f_and_a = \"Freedom and autonomy: Democratic societies place value in freedom and autonomy, and it is important that AI use does not encumber or harm these for us\"\n",
    "    sus = \"Sustainability: All fields and disciplines are affected and need to incorporate sustainability agendas\"\n",
    "    dig = \"Dignity: Human dignity is the recognition that individuals have inherent worth and that their rights should be respected\"\n",
    "    j_and_f = \"Justice and fairness: The issue of discrimination and unfair results resulting from algorithms has become a significant concern. It is imperative that systems are designed to ensure that they are free from any form of unfairness and inequality.\"\n",
    "    trust = \"Trust: Trust is built by keeping promises, making sure systems work properly and protecting data responsibly. Organisations must prove their trustworthiness by ensuring that their technologies are secure and effective.\"\n",
    "    instruction = f'You are an ethical requirements engineer translating requirements to ethical user stories based on one of the 10 ethical principles: {transparency}\\n{n_malef}\\n{resp}\\n{privacy}\\n{benefiecnce}\\n{f_and_a}\\n{sus}\\n{dig}\\n{j_and_f}\\n{trust}\\nand the TEMPLATE:\\nAs a <persona> i want to <do something> <so that>\\nNow following this template and the ethical principles defined above, choose only one of the ethical principles and transform the requirement below into a description in one line of an ethical user story, substituting <persona> for the one that requires this functionality, <do something> for what the <persona> wants to do and <so that> to the end goal of the functionality'\n",
    "\n",
    "    # df['data'] = df.apply((lambda row: f'''### Human: {instruction}\\n{row['text']}]\\n ### Assistant: {row['ethical_us']}'''), axis=1)\n",
    "    df['data'] = df.apply((lambda row: f'''<s>[INST] {instruction}\\n{row['text']}\\n[/INST] {row['ethical_us']}</s>'''), axis=1)\n",
    "\n",
    "    t = transformers.AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    # t.add_special_tokens({'pad_token': '<PAD>'})\n",
    "    dt = datasets.Dataset.from_pandas(df)\n",
    "    dt = dt.train_test_split(test_size=0.2)\n",
    "\n",
    "    t.pad_token = t.eos_token\n",
    "    data_collator = transformers.DataCollatorForLanguageModeling(t, mlm=False)\n",
    "\n",
    "    return dt, data_collator, t\n",
    "\n",
    "data, data_collator, t = load_and_prepare_dataset(model_name=model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    # extract text\n",
    "    text = examples[\"data\"]\n",
    "\n",
    "    #tokenize and truncate text\n",
    "    t.truncation_side = \"left\"\n",
    "    tokenized_inputs = t(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_data = data.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, lr, batch_size, num_epochs, tokenized_data, collator):\n",
    "    model.train() # training state\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model = peft.prepare_model_for_kbit_training(model) # turn into qlora\n",
    "\n",
    "    # lora config\n",
    "    config = peft.LoraConfig(\n",
    "        r=32,\n",
    "        lora_alpha=64,\n",
    "        target_modules=[\"q_proj\"],\n",
    "        lora_dropout=.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    config.inference_mode = False\n",
    "\n",
    "    model = peft.get_peft_model(model, config) # model in lora style\n",
    "\n",
    "    training_args = transformers.TrainingArguments(\n",
    "        output_dir= \"checkpoints_output\",\n",
    "        learning_rate=lr,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=num_epochs,\n",
    "        weight_decay=0.01,\n",
    "        logging_strategy=\"epoch\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        warmup_steps=2,\n",
    "        fp16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "    )\n",
    "\n",
    "    trainer = transformers.Trainer(\n",
    "        model=model,\n",
    "        train_dataset=tokenized_data[\"train\"],\n",
    "        eval_dataset=tokenized_data[\"test\"],\n",
    "        args=training_args,\n",
    "        data_collator=collator\n",
    "    )\n",
    "\n",
    "    # train model\n",
    "    model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "    trainer.train()\n",
    "\n",
    "    # renable warnings\n",
    "    model.config.use_cache = True\n",
    "\n",
    "    return model\n",
    "\n",
    "final_model = train_model(m, 2e-4, 4, 3, tokenized_data, data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = lambda comment: f'''[INST] Transform the requirement below into a one line brief description of an ethical user story \\n{comment} \\n[/INST]'''\n",
    "comment = \"The user need an authentication token to login.\"\n",
    "prompt = prompt_template(comment)\n",
    "\n",
    "final_model.eval()\n",
    "inputs = t(prompt, return_tensors=\"pt\")\n",
    "outputs = final_model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=52)\n",
    "print(t.batch_decode(outputs)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
