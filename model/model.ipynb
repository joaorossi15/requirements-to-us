{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "import peft\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import torch\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a1b126b90dd417fb5392a25fddd2b2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60b4d7b082fc4b14aea317bb7ce550b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json:   0%|          | 0.00/16.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c8ddea0d3034206862c45a640eff27b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c997b57ccddf4410a99663886f1e3463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abbf4bcc1dcb47889a6b33d5d076c544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/4.48G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bba44ab1b384d1eb2265e65c7d0cdb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_model(model_name: str):\n",
    "    # load model\n",
    "    m = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map='cuda',\n",
    "        trust_remote_code=False,\n",
    "        revision='main',\n",
    "    )\n",
    "    \n",
    "    return m.to(device)\n",
    "\n",
    "model_name = 'tiiuae/falcon-7b'\n",
    "m = load_model(model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_dataset(model_name: str):\n",
    "    df = pd.read_csv('/content/data_openai_api_with_mask.csv')\n",
    "    df = df[['text', 'is_major_principle_here', 'ethical_us']]\n",
    "    transparency = \"Transparency: Transparency can typically be understood in two ways: the transparency of the AI technology itself and the transparency of the AI organisations developing and using it. Throughout our analysis, transparency was regularly discussed directly, or in relation to processes required to ensure it, such as explainability, understandability and communication.\"\n",
    "    n_malef = \"Non-maleficence: The principle of non-maleficence gained attention, and in its most basic form, it means to do no harm or avoid doing harm to others\"\n",
    "    resp = \"Responsibility: The principle of responsibility is base on responsibility of the developers and stakeholders over the AI, accountability, liability and acting with integrity\"\n",
    "    privacy = \"Privacy: Related to privacy of personal data, because of the large abundance of data that is required for AI to work, it is important that individuals privacy is not jeopardised as a result\"\n",
    "    benefiecnce = \"Beneficence: Beneficence essentially means to do good, to carry out an activity with the intention of benefitting someone or society as a whole\"\n",
    "    f_and_a = \"Freedom and autonomy: Democratic societies place value in freedom and autonomy, and it is important that AI use does not encumber or harm these for us\"\n",
    "    sus = \"Sustainability: All fields and disciplines are affected and need to incorporate sustainability agendas\"\n",
    "    dig = \"Dignity: Human dignity is the recognition that individuals have inherent worth and that their rights should be respected\"\n",
    "    j_and_f = \"Justice and fairness: The issue of discrimination and unfair results resulting from algorithms has become a significant concern. It is imperative that systems are designed to ensure that they are free from any form of unfairness and inequality.\"\n",
    "    trust = \"Trust: Trust is built by keeping promises, making sure systems work properly and protecting data responsibly. Organisations must prove their trustworthiness by ensuring that their technologies are secure and effective.\"\n",
    "    instruction = f'You are an ethical requirements engineer translating requirements to ethical user stories based on one of the 10 ethical principles:  {transparency}\\n {n_malef}\\n {resp}\\n {privacy}\\n {benefiecnce}\\n {f_and_a}\\n {sus}\\n {dig}\\n {j_and_f}\\n {trust}\\n  and the TEMPLATE:\\n As a <persona> i want to <do something> <so that> \\n\\n Now following this template and the ethical principles definition choose one of the ethical principles and transform the requirement below into a brief description of an ethical user story based on the choosen ethical principle, substituting <persona> for the one that requires this functionality, <do something> for what the <persona> wants to do and <so that> to the end goal of the functionality'\n",
    "\n",
    "    df['data'] = df.apply((lambda row: f'''### Human: {instruction} \\n{row['text']} \\n  ### Assistant: {row['ethical_us']}'''), axis=1)\n",
    "    \n",
    "    t = transformers.AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    t.add_special_tokens({'pad_token': '<PAD>'})\n",
    "    dt = datasets.Dataset.from_pandas(df)\n",
    "    dt = dt.train_test_split(test_size=0.3)\n",
    "\n",
    "    t.pad_token = t.eos_token\n",
    "    data_collator = transformers.DataCollatorForLanguageModeling(t, mlm=False)\n",
    "\n",
    "    return dt, data_collator, t\n",
    "\n",
    "data, data_collator, t = load_and_prepare_dataset(model_name=model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d304ca6c4bf46059b5b427d82773e06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/765 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eb2622febd94e9e9fbff334dadd277d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/328 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    # extract text\n",
    "    text = examples[\"data\"]\n",
    "\n",
    "    #tokenize and truncate text\n",
    "    t.truncation_side = \"left\"\n",
    "    tokenized_inputs = t(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_data = data.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='621' max='1920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 621/1920 7:50:13 < 16:26:47, 0.02 it/s, Epoch 3.23/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.038800</td>\n",
       "      <td>3.020001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.038100</td>\n",
       "      <td>3.020001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.038600</td>\n",
       "      <td>3.020001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, lr, batch_size, num_epochs, tokenized_data, collator):\n",
    "    model.train() # training state\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model = peft.prepare_model_for_kbit_training(model) # turn into qlora\n",
    "\n",
    "    # lora config\n",
    "    config = peft.LoraConfig(\n",
    "        r=32,\n",
    "        lora_alpha=64,\n",
    "        target_modules=[\"q_proj\"],\n",
    "        lora_dropout=.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    config.inference_mode = False\n",
    "\n",
    "    model = peft.get_peft_model(model, config) # model in lora style\n",
    "\n",
    "    training_args = transformers.TrainingArguments(\n",
    "        output_dir= \"checkpoints_output\",\n",
    "        learning_rate=lr,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=num_epochs,\n",
    "        # weight_decay=0.01,\n",
    "        logging_strategy=\"epoch\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        warmup_steps=2,\n",
    "        fp16=False,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "    )\n",
    "\n",
    "    trainer = transformers.Trainer(\n",
    "        model=model,\n",
    "        train_dataset=tokenized_data[\"train\"],\n",
    "        eval_dataset=tokenized_data[\"test\"],\n",
    "        args=training_args,\n",
    "        data_collator=collator\n",
    "    )\n",
    "\n",
    "    # train model\n",
    "    model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "    trainer.train()\n",
    "\n",
    "    # renable warnings\n",
    "    model.config.use_cache = True\n",
    "\n",
    "    return model\n",
    "\n",
    "final_model = train_model(m, 2e-4, 4, 10, tokenized_data, data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] You are a ethical requirements engineer translating requirements to ethical user stories based on 10 ethical principles of: Transparency, Non-maleficence Responsibility, Privacy, Beneficence, Freedom and autonomy, Sustainability, Dignity, Justice and Trustand the template listed below\n",
      " TEMPLATE:\n",
      " As a [persona] i want to [do something] [so that] \n",
      "\n",
      " Now following this template and the ethical principles definition, transform the requirement below into a brief description of an ethical user story, substituing [persona] for the one that requires this functionality, [do something] for what the [persona] wants to do and [so that] to the end goal of the functionality \n",
      "The system shall show the meetings to the user in ascending time order by default. \n",
      "[/INST] As a user who values Transparency and Beneficence, I want the system to display the meetings in ascending time order by default, so that I can easily understand the scheduling of my events and make informed decisions about my time, while ensuring that the system is not misleading or hiding any important information from me.</s>\n"
     ]
    }
   ],
   "source": [
    "transparency = \"Transparency: Transparency can typically be understood in two ways: the transparency of the AI technology itself and the transparency of the AI organisations developing and using it. Throughout our analysis, transparency was regularly discussed directly, or in relation to processes required to ensure it, such as explainability, understandability and communication.\"\n",
    "n_malef = \"Non-maleficence: The principle of non-maleficence gained attention, and in its most basic form, it means to do no harm or avoid doing harm to others\"\n",
    "resp = \"Responsibility: The principle of responsibility is base on responsibility of the developers and stakeholders over the AI, accountability, liability and acting with integrity\"\n",
    "privacy = \"Privacy: Related to privacy of personal data, because of the large abundance of data that is required for AI to work, it is important that individuals privacy is not jeopardised as a result\"\n",
    "benefiecnce = \"Beneficence: Beneficence essentially means to do good, to carry out an activity with the intention of benefitting someone or society as a whole\"\n",
    "f_and_a = \"Freedom and autonomy: Democratic societies place value in freedom and autonomy, and it is important that AI use does not encumber or harm these for us\"\n",
    "sus = \"Sustainability: All fields and disciplines are affected and need to incorporate sustainability agendas\"\n",
    "dig = \"Dignity: Human dignity is the recognition that individuals have inherent worth and that their rights should be respected\"\n",
    "j_and_f = \"Justice and fairness: The issue of discrimination and unfair results resulting from algorithms has become a significant concern. It is imperative that systems are designed to ensure that they are free from any form of unfairness and inequality.\"\n",
    "trust = \"Trust: Trust is built by keeping promises, making sure systems work properly and protecting data responsibly. Organisations must prove their trustworthiness by ensuring that their technologies are secure and effective.\"\n",
    "instruction = f'You are an ethical requirements engineer translating requirements to ethical user stories based on one of the 10 ethical principles:  {transparency}\\n {n_malef}\\n {resp}\\n {privacy}\\n {benefiecnce}\\n {f_and_a}\\n {sus}\\n {dig}\\n {j_and_f}\\n {trust}\\n  and the TEMPLATE:\\n As a <persona> i want to <do something> <so that> \\n\\n Now following this template and the ethical principles definition choose one of the ethical principles and transform the requirement below into a brief description of an ethical user story based on the choosen ethical principle, substituting <persona> for the one that requires this functionality, <do something> for what the <persona> wants to do and <so that> to the end goal of the functionality'\n",
    "\n",
    "prompt_template = lambda comment: f'''### Human: {instruction} \\n{comment} \\n'''\n",
    "comment = \"The system shall show the meetings to the user in ascending time order by default.\"\n",
    "prompt = prompt_template(comment)\n",
    "\n",
    "final_model.eval()\n",
    "inputs = t(prompt, return_tensors=\"pt\")\n",
    "outputs = final_model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=1024)\n",
    "print(t.batch_decode(outputs)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
